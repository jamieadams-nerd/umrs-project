# History of Multilevel Security Systems (MLS)

A Multilevel Security (MLS) system is designed for situations where different people are allowed to see and work with different kinds of information, but all must use the same computer system. For example, one person may be authorized to access highly sensitive information, while another may only be permitted to view public or restricted data. An MLS system ensures that each person can access only the information they are allowed to see, and that sensitive information cannot accidentally or improperly flow from one user, application, or process to another. Rather than relying on people or software to always “do the right thing,” the system itself continuously enforces these rules, even when mistakes occur.

A natural question is why these different kinds of information must exist on the same system at all, instead of simply using separate computers. In practice, completely separating systems is often impractical or impossible. Computing systems are expensive to build, operate, and secure, and people frequently need to work with related information at multiple sensitivity levels as part of a single task. Analysts, engineers, and operators may need to reference public data, internal documents, and sensitive material together to do their jobs effectively. MLS systems exist to make this shared use possible while still strictly controlling access and preventing sensitive information from leaking where it does not belong.

## Origins of MLS 

Multilevel Security (MLS) systems did not emerge as an academic exercise, nor were they conceived as a general-purpose mechanism to make computing systems “more secure.” Instead, they arose from a very specific and practical problem: how to safely operate a single computing system while simultaneously handling information of differing sensitivity levels without accidental or unauthorized disclosure.

To meet this need, purpose-built security models were required. It was no longer sufficient to rely on user judgment or application correctness. Multiple users, processes, and applications had to coexist on the same system while processing information governed by strict handling rules, and the system itself had to enforce those rules reliably.

Early secure computing environments—particularly in military and intelligence contexts—operated under several unavoidable constraints:

* Computing systems were expensive and scarce resources
* Information of different sensitivity levels had to reside on the same machine
* Users could not be assumed to self-police perfectly
* Software defects were expected, not exceptional
* Accidental disclosure was considered as serious as malicious compromise

The traditional access control model of the time—focused primarily on “who can read which file”—proved insufficient. That model implicitly assumed several conditions that did not hold in high-risk environments:

* Users always understand the sensitivity of the data they are handling
* Applications never improperly mix or transform data
* Administrators never misconfigure systems

The resulting problems predate modern cloud computing, containerization, and zero-trust architectures by decades. They arose in environments where compromise was not hypothetical, errors were inevitable, and the consequences of disclosure were severe.

Importantly, MLS did not originate with Linux, SELinux, or even UNIX. Its conceptual foundations reach back to the earliest days of shared computing, when organizations first recognized that multiple users and multiple classification levels would need to coexist on the same physical hardware.

At its core, Multilevel Security was designed to enforce information flow rules even when users, applications, or administrators made mistakes.

'''

## The Original Problem

During the 1960s, large mainframe computers became shared resources. Governments and major institutions began consolidating workloads for many users onto a single system in order to maximize the value of these expensive machines.

At the same time, formal information classification systems were taking shape, particularly within the U.S. Department of Defense. Information was no longer treated as simply “secret” or “not secret,” but organized into hierarchical sensitivity levels and compartmented categories.

This convergence created an immediate and fundamental problem: how can information at one classification level be prevented from leaking to users or processes operating at another level when they all share the same system?

Early operating systems relied almost entirely on discretionary access control (DAC), in which users controlled permissions on the files and resources they owned. In classified environments, this model failed almost immediately, as it placed critical security decisions in the hands of users and applications that could not be assumed to act perfectly or consistently.

## 1973: THE ANDERSON REPORT

One of the foundational documents in MLS history is:
[quote]
----
“Computer Security Technology Planning Study” (1973)
Author: James P. Anderson
Commissioned by: U.S. Air Force
----

This document, commonly called the Anderson Report, formally identified the need for:

* Mandatory access control
* System-enforced security policy
* Protection against both malicious and accidental disclosure

The report made it clear that relying on users or applications to enforce policy was insufficient.


## Mid–1970s: MULTICS AND EARLY MLS IDEAS
MULTICS (Multiplexed Information and Computing Service) was one of the earliest systems to seriously explore strong isolation and security concepts.

Developed by:

* MIT
* Bell Labs
* General Electric

MULTICS introduced ideas that would later become central to MLS, including:

* Ring-based protection
* Fine-grained access controls
* Hardware-assisted isolation

While MULTICS was not a full MLS system in the modern sense, it strongly influenced later secure OS research.


## 1980s: FORMAL MODELS AND TRUSTED SYSTEMS

The 1980s saw the formalization of MLS concepts into mathematical security models.

1983: Bell–LaPadula Model

Developed by:

* David Elliott Bell
* Leonard J. LaPadula

This model introduced formal rules for confidentiality, including:

* Simple Security Property (“no read up”)
* Star Property (“no write down”)

Bell–LaPadula became the conceptual foundation for many MLS systems, particularly those focused on confidentiality rather than integrity. During this time, the U.S. Department of Defense introduced the Trusted Computer System Evaluation Criteria (TCSEC), commonly known as: “The Orange Book” (1983) This defined evaluation classes for secure systems, including requirements for MLS enforcement at higher assurance levels (B1, B2, B3, A1).


## Late 1980s–1990s: MLS UNIX SYSTEMS

Several UNIX systems were extended or re-engineered to meet Orange Book requirements:

* Trusted Solaris (Sun Microsystems)
* HP-UX with Trusted Computing Base extensions
* AIX Trusted Computing Base
* Secure Xenix
* Digital UNIX / OSF/1 with trusted variants
* DG/UX (Data General UNIX)

These systems implemented:

* Security labels
* Mandatory access control
* Trusted path mechanisms
* Strong auditing
* TCB separation

However, these systems were expensive, complex, and often limited to government environments.

DG/UX and Digital UNIX systems often included “trusted” operating modes or optional trusted extensions, designed to meet C2, B1, or higher requirements depending on configuration and evaluation scope. If you worked on DG/UX, it is very likely you encountered:

* Trusted login mechanisms
* Label-aware tools
* Restricted administrative paths
* MLS-style enforcement



## 1990s: FLASK AND THE SHIFT TO FLEXIBLE MAC

One of the most important developments occurred in the mid-to-late 1990s: FLASK (Flux Advanced Security Kernel). This was developed by the University of Utah with NSA involvement.

FLASK introduced a critical idea: Separate security policy from the kernel. Instead of hardcoding MLS logic into the kernel, FLASK defined a framework where:

* The kernel enforces decisions
* A policy engine defines the rules
* Different policies (MLS, type enforcement, etc.) can coexist

This was a major architectural shift and directly influenced SELinux.


## Late 1990s–Early 2000s: SELINUX

SELinux (Security-Enhanced Linux) began as an NSA research project in the late 1990s.

Key milestones:

* 2000: First public SELinux releases
* 2003: SELinux integrated into the Linux kernel
* 2005–2006: Adoption by Fedora and Red Hat Enterprise Linux

SELinux was built on FLASK concepts and supported multiple policy models:

* Type Enforcement (TE)
* Role-Based Access Control (RBAC)
* Multi-Level Security (MLS)
• Multi-Category Security (MCS)

Importantly, SELinux allowed MLS to exist alongside more operationally friendly policies.


## 2000s–2010s: TARGETED POLICY BECOMES THE NORM

While full MLS policies existed in SELinux, most distributions adopted the “targeted” policy by default.

Reasons included:

* Lower operational complexity
* Fewer labeled users
* Easier administration
* Incremental confinement of services

Targeted policy still used mandatory access control, but focused on:

* Confined daemons
* Type-based separation
* Least privilege enforcement

MLS remained available, but primarily in specialized environments.

'''

# Current Mainframe MLS-Capable Operating Systems 

There are MLS-capable operating systems on mainframes, but MLS on mainframes looks different than SELinux MLS, and in many cases it is implemented as labeled security and compartments rather than classic Bell–LaPadula semantics exposed to users.

The dominant example is the IBM z/OS (Mainframe). It has supported labeled security and mandatory-style controls for decades, primarily through:

* RACF (Resource Access Control Facility)
* SECLABELs
* Compartments and categories
* Trusted execution paths for administrative actions

Key points:

* z/OS supports security labels that look very much like MLS labels
* Subjects and objects can be labeled
* Access decisions can consider dominance relationships
* Trusted path concepts are deeply embedded (e.g., secure operator consoles, trusted subsystems)

However:

* z/OS MLS is not usually marketed as “Bell–LaPadula MLS”
* It is integrated into enterprise security and compliance models
* The emphasis is on compartmentalization, auditing, and trusted administration, not research purity

In other words: the MLS ideas survived, but they were adapted to enterprise realities.


## Other Mainframe / Large-System OSes

Historically, Honeywell/Bull GCOS variants had strong security models. Some proprietary defense-oriented systems have implemented MLS internally. MLS as a formal exposed model is rare on modern mainframes, but mandatory enforcement and trusted paths absolutely still exist.

Today:

* IBM z/OS is effectively the mainstream example
* Others either folded into enterprise access control or disappeared

'''

# Is SELinux the Only Game in Town?
SELinux is the most widely deployed general-purpose MLS-capable OS today, but it is not the only system with MLS-inspired enforcement. However, it is the last major system where:

* MLS is first-class
* Policy is explicit and inspectable
* The kernel enforces information flow rules
* The model is open and extensible

That combination is rare.


## Research and Development Beyond SELinux

Rather than “next-generation MLS operating systems,” research has focused on applying MLS principles in new places.

### Microkernels and Formally Verified Systems

* seL4 microkernel and INTEGRITY-178B (Green Hills Software)
** Strong isolation guarantees
** Formal verification
** Partitioned execution environments
* Used in avionics, defense, safety-critical systems
* These systems often:
** Enforce information flow implicitly
** Use static partitioning instead of dynamic MLS
** Replace flexibility with mathematical assurance

They are MLS-adjacent in philosophy, but not general-purpose like Linux.


### Information Flow Control (IFC) Research
Academic and experimental systems explored language-level and OS-level information flow.

Examples:

* HiStar (MIT)
* Asbestos
* Flume
* DStar

Key ideas:

* Label data, not just files
* Track information flow dynamically
* Prevent leaks across security boundaries automatically

These systems are intellectually fascinating, but:
• Very complex
• Difficult to deploy
• Poor fit for general-purpose production systems

Many ideas influenced later work, but the systems themselves did not survive.

### Virtualization and Hardware-Enforced Separation

Virtualization and hardware enforced sepration shifts MLS ideas “downward” into hardware and virtualization layers. Modern systems increasingly rely on:

* Hypervisors
* Virtual machines
* Trusted Execution Environments (TEE)
* Hardware roots of trust (TPM, Secure Boot)

Example:
Instead of enforcing “no write down” in one OS, you run separate OS instances at different trust levels. This is simpler operationally, but:

* More expensive
* Less flexible
* Harder to audit end-to-end data flow


## SELinux Survivability
SELinux let administrators apply MLS ideas selectively, which turned out to be the winning strategy. Moreover, SELinux survived because it made several pragmatic choices:

* Policy separated from kernel (FLASK)
* Multiple models supported (TE, MLS, MCS)
* Incremental deployment possible (targeted policy)
* Integrated into mainstream Linux distributions
* Maintained by Red Hat and others over decades

Most MLS research systems failed because they demanded:

* Whole-system adoption
* New programming models
* Significant retraining
* Perfect discipline


